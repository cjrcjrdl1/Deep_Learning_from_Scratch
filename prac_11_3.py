import sys, os

import gradient_simplenet
import two_layer_net

import numpy as np

# Momentum
# v라는 변수를 추가 이는 물리에서 말하는 속도(velocity)에 해당
# 기울기 방향으로 물체가 힘을 받아 가속된다는 물리 법칙을 수식으로 나타낸 것

class Momentum:
    def __init__(self, lr=0.01, momentum = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

            for key in params.keys():
                self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
                params[key] += self.v[key]

# 모멘텀은 SGD에 비해 덜 딱딱하게 smooth하게 하산
# Momentum과 같은 개념으로 특성 있는 optimizer들이 있음
# 현재는 실제 딥러닝 구동 시 Adam을 주로 사용하기는 함
# 주어진 문제 및 데이터셋마다 효율이 좋은 기법들은 다 다름
# 단 SGD는 가장 단순한 기법인 것은 확실함

# 신경망 학습에서 특히 중요한 것은 가중치의 초기값
# 가중치의 초기값을 모두 0으로 설정하면 좋지 않을까?
# 이것은 좋은 아이디어가 아님
# 가중치를 고르게 분포시키지 않고, 무작위로 초기화하는 것이 최적임

# 세부적으로 어떻게 초기화해야 좋은 값을 얻을 수 있을까?
# 은닉층의 활성화값(활성화 함수의 출력 데이터)의 분포를 관찰해서 비교하면 됨
# 출력값이 0이나 1에 가까워지면, 미분값도 0에 가까워짐
# 데이터 중 0이나 1에 치우친 데이터가 들어가서 이에 대한 학습을 하게되면,
# 역전파의 기울기 값이 점점 작아지다가 사라짐
# -> 기울기 소실 문제(gradient vanishing problem) 발생
# w = np.random.randn(node_num, node_num) * 1
# w = np.random.randn(node_num, node_num) * 0.01
# 이렇게 바꿔서 수행하면 그래프가 중간으로 쏠림
# 이건 기울기 소실 문제는 없겠지만, 데이터를 표현하는
# 일련의 표현력이 부족하다고 생각할 수 있음
# 이 결과는 다수의 뉴런이 거의 같은 값을 출력하고 있다는 뜻
# 그럼 뉴런 100개나 1개나 비슷한 결과를 낼 것
# 데이터 정규화(nomalization)도 마찬가지 개념. 이것도 최대한 흩뿌려서 표현한 후 학습

# Xavier 초기값 설정
# Xavier는 초기값의 표준편차가 1/루트n이 되도록 설정하는 기법(n은 앞층 노드 수)
# w=np.random.randn(node_num, node_num) / np.sqrt(node_num)
# Xavier 결론
# 단순 랜덤 분포보다는 훨씬 고르게 분포
# but 활성화 함수가 Sigmoid가 아니고 ReLU일 때에는 Xavier가 안좋다.

# He initialization
# 초기값의 표준편차가 루트 2/n이 되도록 설정하는 기법
# 이것만 바꿔도 ReLU에 대해 큰 차이가 나옴
# ReLU에 대해서는 He 초기화가 가장 좋다

# MNIST 기준 최종결과(뉴런 수 100개, 5층 신경망, 활성화 함수는 ReLU일때)
# 단순 정규분포 기반 초기화는 순전파 시 0 근처로 데이터 밀집
# 그래서 역전파 때 기울기도 매우 작은 값으로 되어 가중치 갱신이 일어나지 않음
# 하지만 Xavier와 He는 데이터가 골고루 퍼지기 때문에 학습이 잘 됨
# He의 학습 속도가 가장 빠름 (가장 먼저 0에 가까운 loss 값에 접근하기 때문)

# 가중치 초기값을 적절히 설정하면
# 각 층의 활성화 값 분포가 적당히 퍼지면서 학습이 원할하게 수행되었음

# 그렇다면 각 층이 활성화를 적당히 퍼뜨리도록 강제 해보면 어떨까 하는 아이디어에서 실현된 기술이 있음
# -> 배치 정규화(Batch normalization)

# 배치 정규화를 쓰는 이유(실험 결과):
# 1. 학습 속도가 빨라진다.
# 2. 초기값에 크게 의존하지 않는다.
# 3. 오버피팅을 억제한다.

# 배치 정규화를 사용하면 아래와 같이 계층이 이루어진다.
# Affine -> Batch Norm -> ReLU
# idea : 학습 시 미니배치를 단위로 정규화를 진행

# 배치 정규화 사용 시 vs 사용하지 않을 시 비교
# 때에 따라 많은 차이가 나서 많이 사용하는 기법이다.

# DL을 포함한 머신러닝(ML)에서는 오버피팅 문제가 자주 일어남
# 오버피팅 : 신경망이 훈련 데이터에만 지나치게 적용되어, 그 외에 데이터에는 제대로 대응하지 못하는 상태
# 오버피팅이 심하면 training dataset에 대한 accurac는 100%인데
# testset에 대한 accurac는 30,40,50대 정도로 그 이하로도 매우 낮게 나오기도 함

# 오버피팅이 주로 일어나는 경우
# 1. 매개변수가 많고 표현력이 높은 모델을 사용할 때
# 2. 훈련 데이터가 적을 때

# 오버피팅을 감소시키는 방법들
# 1. Weight decay(가중치 감소)
# 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하여 오버피팅을 억제하는 방법
# 대표적으로 L2 regularization(규제)가 있음
# 람다𝜆/2 ∥norm(Wt)∥^2 을 추가하는 것을 weight decay(가중치 감소)라고 함

# 2. dropout(드랍아웃)
# 앞에서 데이터 표현력을 높이기 위해 계층을 더 deep하게, 즉 복잡하게 만들어서 하기도 한다 했음
# 그런데 복잡하게 하면 오버피팅에 취약하기도 함
# 이에 대한 또 다른 해결책은 dropout임 (보통 신경망이 복잡할 때 잘 사용함)

# 하이퍼파라미터 최적화
# 하이퍼파라미터의 최적 값이 존재하는 범위를 조금씩 줄여나간다
# 1. 하이퍼파라미터 값의 범위 설정
# 2. 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출
# 3. 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고 검증 데이터로 정확도 평가
# 4. 1단계와 2단계를 특정횟수(100회 등) 반복하여, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.

# 딥러닝은 사실 하이퍼파라미터에 대한 답 자체가 없음
# 그냥 사람들이 경험상 어떤 값이 잘 나오더라 임
# 즉 수학적으로 해당 하이퍼파라미터가 좋은 하이퍼파라미터다 라는 명확한 지표 없음


