import numpy as np

# 손실함수(성능의 나쁨을 나타냄)
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2) # 평균 제곱 오차 방법 사용

# 정답 2
t = [0,0,1,0,0,0,0,0,0,0] #하나만 1 나머지 0인 것을 원 핫 인코딩

# 2일 확률이 높다고 추정(0.6)
y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
print(mean_squared_error(np.array(y), np.array(t))) # 0.097

# 7일 확률이 높다고 추정(0.6)
y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
print(mean_squared_error(np.array(y), np.array(t))) # 0.5975

# 오차가 적을수록 정답과 가까움 즉 첫번째 예시가 답에 가까움

# 교차 엔트로피 오차
# 밑이 e인 자연로그를 사용하는 신경망의 출력 y(k), t(k)는 정답 레이블
# t(k)는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원 핫 인코딩)
# 그래서 실질적인 정답일 때(t(k)가 1일때의 y(k))의 자연로그를 계산하는 식이 된다.
# 정답 레이블이 2가 정답이고 신경망 출력이 0.6이라면 교차 엔트로피 오차는 -log0.6 = 0.51
# 즉 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 됨
# x가 1일때 y는 0이 되고 x가 0에 가까워질수록 y 값은 점점 작아짐
# 반대로 정답일 때의 출력이 작아질수록 오차는 커짐

# 교차 엔트로피 오차
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta)) #여기서 왜 delta를 더할까??
# delta를 더하는 이유는 np.log()함수에 0을 입력시 마이너스 무한대를 뜻하는 -inf가 되어 더이상 계산X
# 아주 작은 값을 더해 0이 되지 않도록 즉, 마이너스 무한대가 발생하지 않도록 한 것

t = [0,0,1,0,0,0,0,0,0,0]
y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
print(cross_entropy_error(np.array(y), np.array(t))) # 교차 엔트로피의 오차는 0.51

y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
print(cross_entropy_error(np.array(y), np.array(t))) # 오답일 경우이므로 2.30

# 즉 첫번째 경우가 정답일 가능성이 높다고 판단


